{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "800f8439",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import oss2\n",
    "import time\n",
    "\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e56c4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openprompt.trainer import ClassificationRunner, GenerationRunner\n",
    "from openprompt.lm_bff_trainer import LMBFFClassificationRunner\n",
    "from re import template\n",
    "from openprompt.pipeline_base import PromptForClassification, PromptForGeneration\n",
    "from openprompt.utils.reproduciblity import set_seed\n",
    "from openprompt import PromptDataLoader\n",
    "from openprompt.prompts import load_template, load_verbalizer, load_template_generator, load_verbalizer_generator\n",
    "from openprompt.data_utils import FewShotSampler\n",
    "from openprompt.utils.logging import config_experiment_dir, init_logger, logger\n",
    "from openprompt.config import get_config, save_config_to_yaml\n",
    "from openprompt.plms import load_plm_from_config\n",
    "from openprompt.data_utils import load_dataset\n",
    "from openprompt.utils.cuda import model_to_device\n",
    "from openprompt.utils.logging import logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "591d96cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_dataloader(dataset, template, tokenizer,tokenizer_wrapper_class, config, split):\n",
    "    dataloader = PromptDataLoader(\n",
    "        dataset = dataset, \n",
    "        template = template, \n",
    "        tokenizer = tokenizer, \n",
    "        tokenizer_wrapper_class=tokenizer_wrapper_class, \n",
    "        batch_size = config[split].batch_size,\n",
    "        shuffle = config[split].shuffle_data,\n",
    "        teacher_forcing = config[split].teacher_forcing if hasattr(config[split],'teacher_forcing') else None,\n",
    "        predict_eos_token = True if config.task == \"generation\" else False,\n",
    "        **config.dataloader\n",
    "    )\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "94b56eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    config, args = get_config()\n",
    "    # init logger, create log dir and set log level, etc.\n",
    "    if args.resume and args.test:\n",
    "        raise Exception(\"cannot use flag --resume and --test together\")\n",
    "    if args.resume or args.test:\n",
    "        config.logging.path = EXP_PATH = args.resume or args.test\n",
    "    else:\n",
    "        EXP_PATH = config_experiment_dir(config)\n",
    "        init_logger(os.path.join(EXP_PATH, \"log.txt\"), config.logging.file_level, config.logging.console_level)\n",
    "        # save config to the logger directory\n",
    "        save_config_to_yaml(config)\n",
    "    \n",
    "\n",
    "    # load dataset. The valid_dataset can be None\n",
    "    train_dataset, valid_dataset, test_dataset, Processor = load_dataset(config, test = args.test is not None or config.learning_setting == 'zero_shot')\n",
    "\n",
    "    # main\n",
    "    if config.learning_setting == 'full':\n",
    "        score = 0         \n",
    "        for i in  range(1):\n",
    "            res = trainer(\n",
    "                EXP_PATH,\n",
    "                config,\n",
    "                Processor,\n",
    "                resume = args.resume,\n",
    "                test = args.test,\n",
    "                train_dataset = train_dataset,\n",
    "                valid_dataset = valid_dataset,\n",
    "                test_dataset = test_dataset,\n",
    "            )\n",
    "            if res > score:\n",
    "                score  =res\n",
    "        logger.info(f\"the best result of test Performance:\" + \"( micro-f1: \" + str(score) + \", accuracy: \" + str(score)+\")\") \n",
    "    elif config.learning_setting == 'few_shot':\n",
    "        if config.few_shot.few_shot_sampling is None:\n",
    "            raise ValueError(\"use few_shot setting but config.few_shot.few_shot_sampling is not specified\")\n",
    "        seeds = config.sampling_from_train.seed\n",
    "        res = 0\n",
    "        for seed in seeds:\n",
    "            if not args.test:\n",
    "                sampler = FewShotSampler(\n",
    "                    num_examples_per_label = config.sampling_from_train.num_examples_per_label,\n",
    "                    also_sample_dev = config.sampling_from_train.also_sample_dev,\n",
    "                    num_examples_per_label_dev = config.sampling_from_train.num_examples_per_label_dev\n",
    "                )\n",
    "                train_sampled_dataset, valid_sampled_dataset = sampler(\n",
    "                    train_dataset = train_dataset,\n",
    "                    valid_dataset = valid_dataset,\n",
    "                    seed = seed\n",
    "                )\n",
    "                result = trainer(\n",
    "                    os.path.join(EXP_PATH, f\"seed-{seed}\"),\n",
    "                    config,\n",
    "                    Processor,\n",
    "                    resume = args.resume,\n",
    "                    test = args.test,\n",
    "                    train_dataset = train_sampled_dataset,\n",
    "                    valid_dataset = valid_sampled_dataset,\n",
    "                    test_dataset = test_dataset,\n",
    "                )\n",
    "            else:\n",
    "                result = trainer(\n",
    "                    os.path.join(EXP_PATH, f\"seed-{seed}\"),\n",
    "                    config,\n",
    "                    Processor,\n",
    "                    test = args.test,\n",
    "                    test_dataset = test_dataset,\n",
    "                )\n",
    "            res += result\n",
    "        res /= len(seeds)\n",
    "    elif config.learning_setting == 'zero_shot':\n",
    "        res = trainer(\n",
    "            EXP_PATH,\n",
    "            config,\n",
    "            Processor,\n",
    "            zero = True,\n",
    "            train_dataset = train_dataset,\n",
    "            valid_dataset = valid_dataset,\n",
    "            test_dataset = test_dataset,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2df13036",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainer(EXP_PATH, config, Processor, train_dataset = None, valid_dataset = None, test_dataset = None, resume = None, test = None, zero = False):\n",
    "    if not os.path.exists(EXP_PATH):\n",
    "        os.mkdir(EXP_PATH)\n",
    "    config.logging.path = EXP_PATH\n",
    "    # set seed\n",
    "    set_seed(config.reproduce.seed)\n",
    "\n",
    "    # load the pretrained models, its model, tokenizer, and config.\n",
    "    plm_model, plm_tokenizer, plm_config, plm_wrapper_class = load_plm_from_config(config)\n",
    "\n",
    "    \n",
    "\n",
    "    # define template and verbalizer\n",
    "    if config.task == \"classification\":\n",
    "        verbalizer = load_verbalizer(config=config, model=plm_model, tokenizer=plm_tokenizer, plm_config=plm_config, classes=Processor.labels)\n",
    "        template_generate_model, template_generate_tokenizer = None, None\n",
    "        if config.classification.auto_t:\n",
    "            template_generate_model, template_generate_tokenizer, template_generate_config = load_plm_from_config(config.template_generator)\n",
    "            template = load_template(config=config, model=template_generate_model, tokenizer=template_generate_tokenizer, plm_config=template_generate_config, verbalizer=verbalizer)\n",
    "\n",
    "        else:\n",
    "            # define prompt\n",
    "            template = load_template(config=config, model=plm_model, tokenizer=plm_tokenizer, plm_config=plm_config)\n",
    "            \n",
    "            # load prompt’s pipeline model\n",
    "        prompt_model = PromptForClassification(plm_model, template, verbalizer, freeze_plm = config.plm.optimize.freeze_para)\n",
    "            \n",
    "    elif config.task == \"generation\":\n",
    "        template = load_template(config=config, model=plm_model, tokenizer=plm_tokenizer, plm_config=plm_config)\n",
    "        prompt_model = PromptForGeneration(plm_model, template, freeze_plm = config.plm.optimize.freeze_para, gen_config = config.generation)\n",
    "    else:\n",
    "        raise NotImplementedError(f\"config.task {config.task} is not implemented yet. Only classification and generation are supported.\")\n",
    "\n",
    "    # process data and get data_loader\n",
    "    train_dataloader = build_dataloader(train_dataset, template, plm_tokenizer, plm_wrapper_class, config, \"train\") if train_dataset else None\n",
    "    valid_dataloader = build_dataloader(valid_dataset, template, plm_tokenizer, plm_wrapper_class, config, \"dev\") if valid_dataset else None\n",
    "    test_dataloader = build_dataloader(test_dataset, template, plm_tokenizer, plm_wrapper_class, config, \"test\") if test_dataset else None\n",
    "\n",
    "    print('train_dataloader:', train_dataloader)\n",
    "    if config.task == \"classification\":\n",
    "        if config.classification.auto_t or config.classification.auto_v:\n",
    "            runner = LMBFFClassificationRunner(train_dataset = train_dataset, \n",
    "                                        valid_dataset = valid_dataset, \n",
    "                                        test_dataset = test_dataset, \n",
    "                                        model= plm_model, \n",
    "                                        tokenizer = plm_tokenizer, \n",
    "                                        template_generate_tokenizer = template_generate_tokenizer,\n",
    "                                        template_generate_model = template_generate_model,\n",
    "                                        initial_template = template,\n",
    "                                        initial_verbalizer = verbalizer,\n",
    "                                        config = config)\n",
    "        else:\n",
    "            runner = ClassificationRunner(\n",
    "                model = prompt_model,\n",
    "                train_dataloader = train_dataloader,\n",
    "                valid_dataloader = valid_dataloader,\n",
    "                test_dataloader = test_dataloader,\n",
    "                config = config\n",
    "            )\n",
    "    elif config.task == \"generation\":\n",
    "        runner = GenerationRunner(\n",
    "            model = prompt_model,\n",
    "            train_dataloader = train_dataloader,\n",
    "            valid_dataloader = valid_dataloader,\n",
    "            test_dataloader = test_dataloader,\n",
    "            config = config\n",
    "        )\n",
    "        \n",
    "    if zero:\n",
    "        res = runner.test()\n",
    "    elif test:\n",
    "        res = runner.test(ckpt = 'best')\n",
    "    elif resume:\n",
    "        res = runner.run(ckpt = 'last')\n",
    "    else:\n",
    "        res = runner.run()\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "785b62b0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[032m2022-02-18 11:19:01,849\u001b[0m INFO] config.save_config_to_yaml Config saved as logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/config.yaml\n",
      "[\u001b[032m2022-02-18 11:19:01,897\u001b[0m INFO] reproduciblity.set_seed Global seed set to 100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset_config.name.lower(): rte\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.decoder.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_classes: 2\n",
      "self.label_words: [[' yes'], [' no']]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[032m2022-02-18 11:19:16,972\u001b[0m INFO] prompt_base.from_file using template: {\"placeholder\":\"text_a\"} {\"placeholder\":\"text_b\"} {\"mask\"} .\n",
      "tokenizing: 2490it [00:02, 859.74it/s]\n",
      "tokenizing: 277it [00:00, 1003.21it/s]\n",
      "tokenizing: 277it [00:00, 1218.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_dataloader: <openprompt.pipeline_base.PromptDataLoader object at 0x7f7179f57a50>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[\u001b[032m2022-02-18 11:19:25,190\u001b[0m INFO] cuda.model_to_device Using DataParallel\n",
      "[\u001b[032m2022-02-18 11:19:25,221\u001b[0m WARNING] trainer.set_stop_criterion num_training_steps set explicitly, num_epochs is not in use.\n",
      "100%|██████████| 39/39 [01:06<00:00,  1.70s/it, loss=0.752]\n",
      "[\u001b[032m2022-02-18 11:20:31,708\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:20:31,710\u001b[0m INFO] trainer.training_epoch Training epoch 0, num_steps 39, avg_loss: 0.7302, total_loss: 28.4771\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.42it/s]\n",
      "[\u001b[032m2022-02-18 11:20:33,790\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4729241877256318), ('accuracy', 0.4729241877256318)])\n",
      "[\u001b[032m2022-02-18 11:20:33,791\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:20:50,157\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:21:17,913\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.68] \n",
      "[\u001b[032m2022-02-18 11:21:51,332\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:21:51,335\u001b[0m INFO] trainer.training_epoch Training epoch 1, num_steps 78, avg_loss: 0.7044, total_loss: 27.4729\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.21it/s]\n",
      "[\u001b[032m2022-02-18 11:21:53,521\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4729241877256318), ('accuracy', 0.4729241877256318)])\n",
      "[\u001b[032m2022-02-18 11:21:53,522\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:22:09,776\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.71] \n",
      "[\u001b[032m2022-02-18 11:22:43,338\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:22:43,340\u001b[0m INFO] trainer.training_epoch Training epoch 2, num_steps 117, avg_loss: 0.7050, total_loss: 27.4933\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.49it/s]\n",
      "[\u001b[032m2022-02-18 11:22:45,382\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.46570397111913364), ('accuracy', 0.4657039711191336)])\n",
      "[\u001b[032m2022-02-18 11:22:45,383\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:23:02,106\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.696]\n",
      "[\u001b[032m2022-02-18 11:23:35,722\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:23:35,725\u001b[0m INFO] trainer.training_epoch Training epoch 3, num_steps 156, avg_loss: 0.6975, total_loss: 27.2010\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.19it/s]\n",
      "[\u001b[032m2022-02-18 11:23:37,917\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 11:23:37,917\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:24:40,707\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:24:55,009\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.23it/s, loss=0.696]\n",
      "[\u001b[032m2022-02-18 11:25:26,850\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:25:26,853\u001b[0m INFO] trainer.training_epoch Training epoch 4, num_steps 195, avg_loss: 0.7007, total_loss: 27.3268\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.87it/s]\n",
      "[\u001b[032m2022-02-18 11:25:28,743\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5342960288808665), ('accuracy', 0.5342960288808665)])\n",
      "[\u001b[032m2022-02-18 11:25:28,744\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:25:45,565\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.628]\n",
      "[\u001b[032m2022-02-18 11:28:05,117\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:28:05,120\u001b[0m INFO] trainer.training_epoch Training epoch 6, num_steps 273, avg_loss: 0.6708, total_loss: 26.1628\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 11:28:07,196\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.6173285198555957), ('accuracy', 0.6173285198555957)])\n",
      "[\u001b[032m2022-02-18 11:28:07,197\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:28:23,847\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:28:51,564\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.733]\n",
      "[\u001b[032m2022-02-18 11:29:25,282\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:29:25,284\u001b[0m INFO] trainer.training_epoch Training epoch 7, num_steps 312, avg_loss: 0.6447, total_loss: 25.1420\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.41it/s]\n",
      "[\u001b[032m2022-02-18 11:29:27,383\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.703971119133574), ('accuracy', 0.703971119133574)])\n",
      "[\u001b[032m2022-02-18 11:29:27,384\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:29:43,982\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:30:11,701\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.494]\n",
      "[\u001b[032m2022-02-18 11:30:45,415\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:30:45,417\u001b[0m INFO] trainer.training_epoch Training epoch 8, num_steps 351, avg_loss: 0.5781, total_loss: 22.5453\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.40it/s]\n",
      "[\u001b[032m2022-02-18 11:30:47,507\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.740072202166065), ('accuracy', 0.740072202166065)])\n",
      "[\u001b[032m2022-02-18 11:30:47,508\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:31:10,144\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:31:25,384\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.486]\n",
      "[\u001b[032m2022-02-18 11:31:59,056\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:31:59,059\u001b[0m INFO] trainer.training_epoch Training epoch 9, num_steps 390, avg_loss: 0.5280, total_loss: 20.5926\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.44it/s]\n",
      "[\u001b[032m2022-02-18 11:32:01,154\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.7075812274368231), ('accuracy', 0.7075812274368231)])\n",
      "[\u001b[032m2022-02-18 11:32:01,155\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:32:17,767\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.374]\n",
      "[\u001b[032m2022-02-18 11:32:51,549\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:32:51,552\u001b[0m INFO] trainer.training_epoch Training epoch 10, num_steps 429, avg_loss: 0.4383, total_loss: 17.0941\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 11:32:53,624\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.7833935018050542), ('accuracy', 0.7833935018050542)])\n",
      "[\u001b[032m2022-02-18 11:32:53,624\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:33:10,503\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:33:38,230\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.356]\n",
      "[\u001b[032m2022-02-18 11:34:11,947\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:34:11,950\u001b[0m INFO] trainer.training_epoch Training epoch 11, num_steps 468, avg_loss: 0.3631, total_loss: 14.1623\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.41it/s]\n",
      "[\u001b[032m2022-02-18 11:34:14,038\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8086642599277978), ('accuracy', 0.8086642599277978)])\n",
      "[\u001b[032m2022-02-18 11:34:14,039\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:34:30,925\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:34:58,644\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.562]\n",
      "[\u001b[032m2022-02-18 11:35:32,251\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:35:32,253\u001b[0m INFO] trainer.training_epoch Training epoch 12, num_steps 507, avg_loss: 0.3065, total_loss: 11.9539\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.46it/s]\n",
      "[\u001b[032m2022-02-18 11:35:34,320\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.7617328519855595), ('accuracy', 0.7617328519855595)])\n",
      "[\u001b[032m2022-02-18 11:35:34,321\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:35:50,917\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.283]\n",
      "[\u001b[032m2022-02-18 11:36:24,414\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:36:24,417\u001b[0m INFO] trainer.training_epoch Training epoch 13, num_steps 546, avg_loss: 0.2945, total_loss: 11.4859\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.24it/s]\n",
      "[\u001b[032m2022-02-18 11:36:26,577\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8267148014440433), ('accuracy', 0.8267148014440433)])\n",
      "[\u001b[032m2022-02-18 11:36:26,578\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:36:42,870\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:37:10,598\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.25]  \n",
      "[\u001b[032m2022-02-18 11:37:44,339\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:37:44,342\u001b[0m INFO] trainer.training_epoch Training epoch 14, num_steps 585, avg_loss: 0.2156, total_loss: 8.4077\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.33it/s]\n",
      "[\u001b[032m2022-02-18 11:37:46,465\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8014440433212996), ('accuracy', 0.8014440433212996)])\n",
      "[\u001b[032m2022-02-18 11:37:46,466\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:38:03,149\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.147] \n",
      "[\u001b[032m2022-02-18 11:38:35,115\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:38:35,115\u001b[0m INFO] trainer.training_epoch Training epoch 15, num_steps 624, avg_loss: 0.1667, total_loss: 6.4994\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.83it/s]\n",
      "[\u001b[032m2022-02-18 11:38:37,022\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8267148014440433), ('accuracy', 0.8267148014440433)])\n",
      "[\u001b[032m2022-02-18 11:38:37,025\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:38:53,961\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.23it/s, loss=0.166] \n",
      "[\u001b[032m2022-02-18 11:39:25,722\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:39:25,723\u001b[0m INFO] trainer.training_epoch Training epoch 16, num_steps 663, avg_loss: 0.1459, total_loss: 5.6902\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.88it/s]\n",
      "[\u001b[032m2022-02-18 11:39:27,612\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8267148014440433), ('accuracy', 0.8267148014440433)])\n",
      "[\u001b[032m2022-02-18 11:39:27,615\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:39:44,501\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.197] \n",
      "[\u001b[032m2022-02-18 11:40:18,076\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:40:18,078\u001b[0m INFO] trainer.training_epoch Training epoch 17, num_steps 702, avg_loss: 0.1328, total_loss: 5.1793\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.22it/s]\n",
      "[\u001b[032m2022-02-18 11:40:20,246\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8267148014440433), ('accuracy', 0.8267148014440433)])\n",
      "[\u001b[032m2022-02-18 11:40:20,247\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:40:36,558\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.125] \n",
      "[\u001b[032m2022-02-18 11:41:10,219\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:41:10,221\u001b[0m INFO] trainer.training_epoch Training epoch 18, num_steps 741, avg_loss: 0.0999, total_loss: 3.8976\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.40it/s]\n",
      "[\u001b[032m2022-02-18 11:41:12,321\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.7942238267148014), ('accuracy', 0.7942238267148014)])\n",
      "[\u001b[032m2022-02-18 11:41:12,322\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:41:50,397\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0657]\n",
      "[\u001b[032m2022-02-18 11:42:24,130\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:42:24,132\u001b[0m INFO] trainer.training_epoch Training epoch 19, num_steps 780, avg_loss: 0.0967, total_loss: 3.7731\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.22it/s]\n",
      "[\u001b[032m2022-02-18 11:42:26,306\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.819494584837545), ('accuracy', 0.8194945848375451)])\n",
      "[\u001b[032m2022-02-18 11:42:26,307\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:42:42,895\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.0383]\n",
      "[\u001b[032m2022-02-18 11:43:16,335\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:43:16,338\u001b[0m INFO] trainer.training_epoch Training epoch 20, num_steps 819, avg_loss: 0.0735, total_loss: 2.8649\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.41it/s]\n",
      "[\u001b[032m2022-02-18 11:43:18,419\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8158844765342961), ('accuracy', 0.8158844765342961)])\n",
      "[\u001b[032m2022-02-18 11:43:18,419\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:43:36,118\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0279]\n",
      "[\u001b[032m2022-02-18 11:44:09,724\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:44:09,727\u001b[0m INFO] trainer.training_epoch Training epoch 21, num_steps 858, avg_loss: 0.1123, total_loss: 4.3799\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.24it/s]\n",
      "[\u001b[032m2022-02-18 11:44:11,895\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8122743682310469), ('accuracy', 0.8122743682310469)])\n",
      "[\u001b[032m2022-02-18 11:44:11,895\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:44:28,532\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0493]\n",
      "[\u001b[032m2022-02-18 11:45:02,241\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:45:02,244\u001b[0m INFO] trainer.training_epoch Training epoch 22, num_steps 897, avg_loss: 0.0699, total_loss: 2.7257\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.33it/s]\n",
      "[\u001b[032m2022-02-18 11:45:04,359\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8050541516245487), ('accuracy', 0.8050541516245487)])\n",
      "[\u001b[032m2022-02-18 11:45:04,360\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:45:20,948\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.00942]\n",
      "[\u001b[032m2022-02-18 11:45:54,572\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:45:54,575\u001b[0m INFO] trainer.training_epoch Training epoch 23, num_steps 936, avg_loss: 0.0616, total_loss: 2.4038\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.44it/s]\n",
      "[\u001b[032m2022-02-18 11:45:56,656\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8158844765342961), ('accuracy', 0.8158844765342961)])\n",
      "[\u001b[032m2022-02-18 11:45:56,657\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:46:12,770\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.0792] \n",
      "[\u001b[032m2022-02-18 11:46:44,642\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:46:44,644\u001b[0m INFO] trainer.training_epoch Training epoch 24, num_steps 975, avg_loss: 0.0641, total_loss: 2.5014\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.93it/s]\n",
      "[\u001b[032m2022-02-18 11:46:46,507\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8231046931407943), ('accuracy', 0.8231046931407943)])\n",
      "[\u001b[032m2022-02-18 11:46:46,507\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:47:10,031\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.22it/s, loss=0.196] \n",
      "[\u001b[032m2022-02-18 11:47:42,055\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:47:42,058\u001b[0m INFO] trainer.training_epoch Training epoch 25, num_steps 1014, avg_loss: 0.0683, total_loss: 2.6638\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.48it/s]\n",
      "[\u001b[032m2022-02-18 11:47:44,113\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8411552346570397), ('accuracy', 0.8411552346570397)])\n",
      "[\u001b[032m2022-02-18 11:47:44,113\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:48:00,682\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:48:28,396\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.0143] \n",
      "[\u001b[032m2022-02-18 11:49:01,629\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:49:01,631\u001b[0m INFO] trainer.training_epoch Training epoch 26, num_steps 1053, avg_loss: 0.0544, total_loss: 2.1218\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.38it/s]\n",
      "[\u001b[032m2022-02-18 11:49:03,735\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8375451263537906), ('accuracy', 0.8375451263537906)])\n",
      "[\u001b[032m2022-02-18 11:49:03,736\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:49:21,435\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0381] \n",
      "[\u001b[032m2022-02-18 11:49:55,098\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:49:55,100\u001b[0m INFO] trainer.training_epoch Training epoch 27, num_steps 1092, avg_loss: 0.0516, total_loss: 2.0135\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.22it/s]\n",
      "[\u001b[032m2022-02-18 11:49:57,278\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8447653429602888), ('accuracy', 0.8447653429602888)])\n",
      "[\u001b[032m2022-02-18 11:49:57,279\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:50:13,882\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:50:29,166\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.18it/s, loss=0.0419] \n",
      "[\u001b[032m2022-02-18 11:51:02,116\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:51:02,118\u001b[0m INFO] trainer.training_epoch Training epoch 28, num_steps 1131, avg_loss: 0.0443, total_loss: 1.7261\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.58it/s]\n",
      "[\u001b[032m2022-02-18 11:51:04,123\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.7870036101083032), ('accuracy', 0.7870036101083032)])\n",
      "[\u001b[032m2022-02-18 11:51:04,124\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:51:20,016\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0522] \n",
      "[\u001b[032m2022-02-18 11:51:53,714\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:51:53,716\u001b[0m INFO] trainer.training_epoch Training epoch 29, num_steps 1170, avg_loss: 0.0619, total_loss: 2.4138\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.19it/s]\n",
      "[\u001b[032m2022-02-18 11:51:55,911\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.855595667870036), ('accuracy', 0.855595667870036)])\n",
      "[\u001b[032m2022-02-18 11:51:55,912\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:52:12,212\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:53:09,990\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0506]\n",
      "[\u001b[032m2022-02-18 11:53:43,755\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:53:43,757\u001b[0m INFO] trainer.training_epoch Training epoch 30, num_steps 1209, avg_loss: 0.0648, total_loss: 2.5291\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.38it/s]\n",
      "[\u001b[032m2022-02-18 11:53:45,856\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8339350180505415), ('accuracy', 0.8339350180505415)])\n",
      "[\u001b[032m2022-02-18 11:53:45,856\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:54:02,508\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.226]  \n",
      "[\u001b[032m2022-02-18 11:54:36,160\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:54:36,162\u001b[0m INFO] trainer.training_epoch Training epoch 31, num_steps 1248, avg_loss: 0.0668, total_loss: 2.6046\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.44it/s]\n",
      "[\u001b[032m2022-02-18 11:54:38,226\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8447653429602888), ('accuracy', 0.8447653429602888)])\n",
      "[\u001b[032m2022-02-18 11:54:38,227\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:54:54,633\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.0251]\n",
      "[\u001b[032m2022-02-18 11:55:28,254\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:55:28,256\u001b[0m INFO] trainer.training_epoch Training epoch 32, num_steps 1287, avg_loss: 0.0527, total_loss: 2.0563\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.42it/s]\n",
      "[\u001b[032m2022-02-18 11:55:30,351\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8339350180505415), ('accuracy', 0.8339350180505415)])\n",
      "[\u001b[032m2022-02-18 11:55:30,352\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:56:19,536\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.0231] \n",
      "[\u001b[032m2022-02-18 11:56:53,010\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:56:53,013\u001b[0m INFO] trainer.training_epoch Training epoch 33, num_steps 1326, avg_loss: 0.0541, total_loss: 2.1108\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.46it/s]\n",
      "[\u001b[032m2022-02-18 11:56:55,074\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.855595667870036), ('accuracy', 0.855595667870036)])\n",
      "[\u001b[032m2022-02-18 11:56:55,075\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:57:11,658\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.21it/s, loss=0.085] \n",
      "[\u001b[032m2022-02-18 11:57:43,896\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:57:43,899\u001b[0m INFO] trainer.training_epoch Training epoch 34, num_steps 1365, avg_loss: 0.0742, total_loss: 2.8922\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.76it/s]\n",
      "[\u001b[032m2022-02-18 11:57:45,833\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8592057761732852), ('accuracy', 0.8592057761732852)])\n",
      "[\u001b[032m2022-02-18 11:57:45,834\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:58:02,421\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:58:17,693\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.19it/s, loss=0.0898] \n",
      "[\u001b[032m2022-02-18 11:58:50,571\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:58:50,572\u001b[0m INFO] trainer.training_epoch Training epoch 35, num_steps 1404, avg_loss: 0.0420, total_loss: 1.6378\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.49it/s]\n",
      "[\u001b[032m2022-02-18 11:58:52,614\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8844765342960289), ('accuracy', 0.8844765342960289)])\n",
      "[\u001b[032m2022-02-18 11:58:52,615\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 11:59:10,371\u001b[0m INFO] trainer.save_checkpoint Copying checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt to logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 11:59:25,675\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.0781]\n",
      "[\u001b[032m2022-02-18 11:59:59,547\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 11:59:59,549\u001b[0m INFO] trainer.training_epoch Training epoch 36, num_steps 1443, avg_loss: 0.0788, total_loss: 3.0741\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.45it/s]\n",
      "[\u001b[032m2022-02-18 12:00:01,614\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8231046931407943), ('accuracy', 0.8231046931407943)])\n",
      "[\u001b[032m2022-02-18 12:00:01,614\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:00:17,906\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.192] \n",
      "[\u001b[032m2022-02-18 12:00:51,474\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:00:51,477\u001b[0m INFO] trainer.training_epoch Training epoch 37, num_steps 1482, avg_loss: 0.1163, total_loss: 4.5376\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.21it/s]\n",
      "[\u001b[032m2022-02-18 12:00:53,680\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8267148014440433), ('accuracy', 0.8267148014440433)])\n",
      "[\u001b[032m2022-02-18 12:00:53,681\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:01:11,368\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.19it/s, loss=0.0415]\n",
      "[\u001b[032m2022-02-18 12:01:44,260\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:01:44,262\u001b[0m INFO] trainer.training_epoch Training epoch 38, num_steps 1521, avg_loss: 0.1303, total_loss: 5.0818\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.66it/s]\n",
      "[\u001b[032m2022-02-18 12:01:46,242\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8411552346570397), ('accuracy', 0.8411552346570397)])\n",
      "[\u001b[032m2022-02-18 12:01:46,243\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:02:03,921\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.125] \n",
      "[\u001b[032m2022-02-18 12:02:37,405\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:02:37,407\u001b[0m INFO] trainer.training_epoch Training epoch 39, num_steps 1560, avg_loss: 0.1096, total_loss: 4.2748\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.20it/s]\n",
      "[\u001b[032m2022-02-18 12:02:39,622\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8772563176895307), ('accuracy', 0.8772563176895307)])\n",
      "[\u001b[032m2022-02-18 12:02:39,623\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:02:57,383\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.106] \n",
      "[\u001b[032m2022-02-18 12:03:31,241\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:03:31,243\u001b[0m INFO] trainer.training_epoch Training epoch 40, num_steps 1599, avg_loss: 0.1660, total_loss: 6.4721\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.33it/s]\n",
      "[\u001b[032m2022-02-18 12:03:33,404\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.819494584837545), ('accuracy', 0.8194945848375451)])\n",
      "[\u001b[032m2022-02-18 12:03:33,405\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:03:51,257\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.195] \n",
      "[\u001b[032m2022-02-18 12:04:25,151\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:04:25,153\u001b[0m INFO] trainer.training_epoch Training epoch 41, num_steps 1638, avg_loss: 0.1612, total_loss: 6.2852\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.37it/s]\n",
      "[\u001b[032m2022-02-18 12:04:27,291\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.8014440433212996), ('accuracy', 0.8014440433212996)])\n",
      "[\u001b[032m2022-02-18 12:04:27,292\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:04:44,376\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.21it/s, loss=0.461] \n",
      "[\u001b[032m2022-02-18 12:05:16,499\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:05:16,500\u001b[0m INFO] trainer.training_epoch Training epoch 42, num_steps 1677, avg_loss: 0.2865, total_loss: 11.1744\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.75it/s]\n",
      "[\u001b[032m2022-02-18 12:05:18,467\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.740072202166065), ('accuracy', 0.740072202166065)])\n",
      "[\u001b[032m2022-02-18 12:05:18,468\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:05:36,131\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.591]\n",
      "[\u001b[032m2022-02-18 12:06:09,814\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:06:09,817\u001b[0m INFO] trainer.training_epoch Training epoch 43, num_steps 1716, avg_loss: 0.5524, total_loss: 21.5431\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.45it/s]\n",
      "[\u001b[032m2022-02-18 12:06:11,901\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.6534296028880866), ('accuracy', 0.6534296028880866)])\n",
      "[\u001b[032m2022-02-18 12:06:11,902\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:06:29,608\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.662]\n",
      "[\u001b[032m2022-02-18 12:07:03,351\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:07:03,354\u001b[0m INFO] trainer.training_epoch Training epoch 44, num_steps 1755, avg_loss: 0.6480, total_loss: 25.2731\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.42it/s]\n",
      "[\u001b[032m2022-02-18 12:07:05,436\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5667870036101083), ('accuracy', 0.5667870036101083)])\n",
      "[\u001b[032m2022-02-18 12:07:05,437\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:07:23,129\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.665]\n",
      "[\u001b[032m2022-02-18 12:07:55,098\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:07:55,100\u001b[0m INFO] trainer.training_epoch Training epoch 45, num_steps 1794, avg_loss: 0.6862, total_loss: 26.7608\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.86it/s]\n",
      "[\u001b[032m2022-02-18 12:07:57,000\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.49097472924187724), ('accuracy', 0.49097472924187724)])\n",
      "[\u001b[032m2022-02-18 12:07:57,001\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:08:14,670\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.693]\n",
      "[\u001b[032m2022-02-18 12:08:46,611\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:08:46,614\u001b[0m INFO] trainer.training_epoch Training epoch 46, num_steps 1833, avg_loss: 0.6984, total_loss: 27.2359\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.86it/s]\n",
      "[\u001b[032m2022-02-18 12:08:48,507\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5126353790613718), ('accuracy', 0.5126353790613718)])\n",
      "[\u001b[032m2022-02-18 12:08:48,508\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:09:06,181\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.691]\n",
      "[\u001b[032m2022-02-18 12:09:38,171\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:09:38,173\u001b[0m INFO] trainer.training_epoch Training epoch 47, num_steps 1872, avg_loss: 0.6965, total_loss: 27.1616\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.89it/s]\n",
      "[\u001b[032m2022-02-18 12:09:40,063\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:09:40,064\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:09:57,756\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.19it/s, loss=0.688]\n",
      "[\u001b[032m2022-02-18 12:10:30,583\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:10:30,585\u001b[0m INFO] trainer.training_epoch Training epoch 48, num_steps 1911, avg_loss: 0.6971, total_loss: 27.1887\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.54it/s]\n",
      "[\u001b[032m2022-02-18 12:10:32,624\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.49458483754512633), ('accuracy', 0.49458483754512633)])\n",
      "[\u001b[032m2022-02-18 12:10:32,625\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:10:50,319\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.682]\n",
      "[\u001b[032m2022-02-18 12:11:24,145\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:11:24,148\u001b[0m INFO] trainer.training_epoch Training epoch 49, num_steps 1950, avg_loss: 0.6957, total_loss: 27.1342\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.18it/s]\n",
      "[\u001b[032m2022-02-18 12:11:26,348\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5306859205776173), ('accuracy', 0.5306859205776173)])\n",
      "[\u001b[032m2022-02-18 12:11:26,349\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:11:44,076\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.714]\n",
      "[\u001b[032m2022-02-18 12:12:17,783\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:12:17,786\u001b[0m INFO] trainer.training_epoch Training epoch 50, num_steps 1989, avg_loss: 0.6947, total_loss: 27.0914\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.64it/s]\n",
      "[\u001b[032m2022-02-18 12:12:19,768\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:12:19,769\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:12:37,493\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.698]\n",
      "[\u001b[032m2022-02-18 12:13:09,459\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:13:09,462\u001b[0m INFO] trainer.training_epoch Training epoch 51, num_steps 2028, avg_loss: 0.6942, total_loss: 27.0743\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.90it/s]\n",
      "[\u001b[032m2022-02-18 12:13:11,351\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.47653429602888087), ('accuracy', 0.47653429602888087)])\n",
      "[\u001b[032m2022-02-18 12:13:11,352\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:13:29,132\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.23it/s, loss=0.704]\n",
      "[\u001b[032m2022-02-18 12:14:00,890\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:14:00,892\u001b[0m INFO] trainer.training_epoch Training epoch 52, num_steps 2067, avg_loss: 0.6975, total_loss: 27.2012\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.85it/s]\n",
      "[\u001b[032m2022-02-18 12:14:02,786\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.47653429602888087), ('accuracy', 0.47653429602888087)])\n",
      "[\u001b[032m2022-02-18 12:14:02,787\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:14:20,454\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.23it/s, loss=0.698]\n",
      "[\u001b[032m2022-02-18 12:14:52,241\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:14:52,244\u001b[0m INFO] trainer.training_epoch Training epoch 53, num_steps 2106, avg_loss: 0.6960, total_loss: 27.1458\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.57it/s]\n",
      "[\u001b[032m2022-02-18 12:14:54,252\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4981949458483754), ('accuracy', 0.4981949458483754)])\n",
      "[\u001b[032m2022-02-18 12:14:54,253\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:15:11,948\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.692]\n",
      "[\u001b[032m2022-02-18 12:15:43,827\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:15:43,829\u001b[0m INFO] trainer.training_epoch Training epoch 54, num_steps 2145, avg_loss: 0.6966, total_loss: 27.1667\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.89it/s]\n",
      "[\u001b[032m2022-02-18 12:15:45,708\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4729241877256318), ('accuracy', 0.4729241877256318)])\n",
      "[\u001b[032m2022-02-18 12:15:45,709\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:16:03,394\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.693]\n",
      "[\u001b[032m2022-02-18 12:16:35,392\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:16:35,393\u001b[0m INFO] trainer.training_epoch Training epoch 55, num_steps 2184, avg_loss: 0.6975, total_loss: 27.2036\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.54it/s]\n",
      "[\u001b[032m2022-02-18 12:16:37,419\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4729241877256318), ('accuracy', 0.4729241877256318)])\n",
      "[\u001b[032m2022-02-18 12:16:37,419\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:16:55,100\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.694]\n",
      "[\u001b[032m2022-02-18 12:17:28,770\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:17:28,772\u001b[0m INFO] trainer.training_epoch Training epoch 56, num_steps 2223, avg_loss: 0.6943, total_loss: 27.0762\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.42it/s]\n",
      "[\u001b[032m2022-02-18 12:17:30,850\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:17:30,851\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:17:48,573\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.699]\n",
      "[\u001b[032m2022-02-18 12:18:22,282\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:18:22,285\u001b[0m INFO] trainer.training_epoch Training epoch 57, num_steps 2262, avg_loss: 0.6958, total_loss: 27.1343\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.20it/s]\n",
      "[\u001b[032m2022-02-18 12:18:24,471\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.46570397111913364), ('accuracy', 0.4657039711191336)])\n",
      "[\u001b[032m2022-02-18 12:18:24,471\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:18:42,192\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.686]\n",
      "[\u001b[032m2022-02-18 12:19:15,848\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:19:15,851\u001b[0m INFO] trainer.training_epoch Training epoch 58, num_steps 2301, avg_loss: 0.6938, total_loss: 27.0579\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.42it/s]\n",
      "[\u001b[032m2022-02-18 12:19:17,926\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:19:17,927\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:19:35,752\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.22it/s, loss=0.697]\n",
      "[\u001b[032m2022-02-18 12:20:07,783\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:20:07,786\u001b[0m INFO] trainer.training_epoch Training epoch 59, num_steps 2340, avg_loss: 0.6934, total_loss: 27.0414\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.87it/s]\n",
      "[\u001b[032m2022-02-18 12:20:09,679\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5306859205776173), ('accuracy', 0.5306859205776173)])\n",
      "[\u001b[032m2022-02-18 12:20:09,680\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:20:27,502\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.689]\n",
      "[\u001b[032m2022-02-18 12:20:59,364\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:20:59,367\u001b[0m INFO] trainer.training_epoch Training epoch 60, num_steps 2379, avg_loss: 0.6981, total_loss: 27.2261\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.92it/s]\n",
      "[\u001b[032m2022-02-18 12:21:01,232\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:21:01,233\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:21:18,915\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.22it/s, loss=0.683]\n",
      "[\u001b[032m2022-02-18 12:21:50,986\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:21:50,989\u001b[0m INFO] trainer.training_epoch Training epoch 61, num_steps 2418, avg_loss: 0.6971, total_loss: 27.1853\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.84it/s]\n",
      "[\u001b[032m2022-02-18 12:21:52,894\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:21:52,895\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:22:10,583\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.22it/s, loss=0.713]\n",
      "[\u001b[032m2022-02-18 12:22:42,608\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:22:42,610\u001b[0m INFO] trainer.training_epoch Training epoch 62, num_steps 2457, avg_loss: 0.6940, total_loss: 27.0654\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.85it/s]\n",
      "[\u001b[032m2022-02-18 12:22:44,521\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:22:44,522\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:23:02,201\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.686]\n",
      "[\u001b[032m2022-02-18 12:23:34,139\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:23:34,141\u001b[0m INFO] trainer.training_epoch Training epoch 63, num_steps 2496, avg_loss: 0.6941, total_loss: 27.0698\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.88it/s]\n",
      "[\u001b[032m2022-02-18 12:23:36,029\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5306859205776173), ('accuracy', 0.5306859205776173)])\n",
      "[\u001b[032m2022-02-18 12:23:36,030\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:23:53,731\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.22it/s, loss=0.691]\n",
      "[\u001b[032m2022-02-18 12:24:25,791\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:24:25,793\u001b[0m INFO] trainer.training_epoch Training epoch 64, num_steps 2535, avg_loss: 0.6944, total_loss: 27.0799\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.90it/s]\n",
      "[\u001b[032m2022-02-18 12:24:27,670\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.516245487364621), ('accuracy', 0.516245487364621)])\n",
      "[\u001b[032m2022-02-18 12:24:27,671\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:24:44,547\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.18it/s, loss=0.69] \n",
      "[\u001b[032m2022-02-18 12:25:17,635\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:25:17,638\u001b[0m INFO] trainer.training_epoch Training epoch 65, num_steps 2574, avg_loss: 0.6937, total_loss: 27.0553\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.15it/s]\n",
      "[\u001b[032m2022-02-18 12:25:19,849\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4620938628158845), ('accuracy', 0.4620938628158845)])\n",
      "[\u001b[032m2022-02-18 12:25:19,850\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:25:37,547\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.19it/s, loss=0.692]\n",
      "[\u001b[032m2022-02-18 12:26:10,298\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:26:10,301\u001b[0m INFO] trainer.training_epoch Training epoch 66, num_steps 2613, avg_loss: 0.6955, total_loss: 27.1244\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.57it/s]\n",
      "[\u001b[032m2022-02-18 12:26:12,312\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:26:12,313\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:26:29,980\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.689]\n",
      "[\u001b[032m2022-02-18 12:27:03,412\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:27:03,416\u001b[0m INFO] trainer.training_epoch Training epoch 67, num_steps 2652, avg_loss: 0.6958, total_loss: 27.1359\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.18it/s]\n",
      "[\u001b[032m2022-02-18 12:27:05,617\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:27:05,617\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:27:23,404\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.695]\n",
      "[\u001b[032m2022-02-18 12:27:57,215\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:27:57,218\u001b[0m INFO] trainer.training_epoch Training epoch 68, num_steps 2691, avg_loss: 0.6937, total_loss: 27.0540\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.40it/s]\n",
      "[\u001b[032m2022-02-18 12:27:59,301\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:27:59,302\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:28:15,763\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.69] \n",
      "[\u001b[032m2022-02-18 12:28:49,473\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:28:49,475\u001b[0m INFO] trainer.training_epoch Training epoch 69, num_steps 2730, avg_loss: 0.6954, total_loss: 27.1207\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.44it/s]\n",
      "[\u001b[032m2022-02-18 12:28:51,545\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:28:51,545\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:29:08,507\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.702]\n",
      "[\u001b[032m2022-02-18 12:29:42,044\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:29:42,047\u001b[0m INFO] trainer.training_epoch Training epoch 70, num_steps 2769, avg_loss: 0.6944, total_loss: 27.0835\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.46it/s]\n",
      "[\u001b[032m2022-02-18 12:29:44,107\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:29:44,108\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:30:01,819\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.682]\n",
      "[\u001b[032m2022-02-18 12:30:35,639\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:30:35,642\u001b[0m INFO] trainer.training_epoch Training epoch 71, num_steps 2808, avg_loss: 0.6955, total_loss: 27.1255\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 12:30:37,715\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:30:37,715\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:30:54,448\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.692]\n",
      "[\u001b[032m2022-02-18 12:31:28,214\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:31:28,217\u001b[0m INFO] trainer.training_epoch Training epoch 72, num_steps 2847, avg_loss: 0.6937, total_loss: 27.0559\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 12:31:30,287\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:31:30,288\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:31:47,987\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.714]\n",
      "[\u001b[032m2022-02-18 12:32:21,766\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:32:21,769\u001b[0m INFO] trainer.training_epoch Training epoch 73, num_steps 2886, avg_loss: 0.6958, total_loss: 27.1374\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.38it/s]\n",
      "[\u001b[032m2022-02-18 12:32:23,866\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.4729241877256318), ('accuracy', 0.4729241877256318)])\n",
      "[\u001b[032m2022-02-18 12:32:23,867\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:32:40,749\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.693]\n",
      "[\u001b[032m2022-02-18 12:33:14,555\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:33:14,556\u001b[0m INFO] trainer.training_epoch Training epoch 74, num_steps 2925, avg_loss: 0.6946, total_loss: 27.0891\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.40it/s]\n",
      "[\u001b[032m2022-02-18 12:33:16,639\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:33:16,640\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:33:33,535\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.21it/s, loss=0.703]\n",
      "[\u001b[032m2022-02-18 12:34:05,739\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:34:05,742\u001b[0m INFO] trainer.training_epoch Training epoch 75, num_steps 2964, avg_loss: 0.6955, total_loss: 27.1256\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.46it/s]\n",
      "[\u001b[032m2022-02-18 12:34:07,805\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:34:07,806\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:34:24,643\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.701]\n",
      "[\u001b[032m2022-02-18 12:34:58,088\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:34:58,091\u001b[0m INFO] trainer.training_epoch Training epoch 76, num_steps 3003, avg_loss: 0.6944, total_loss: 27.0821\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.56it/s]\n",
      "[\u001b[032m2022-02-18 12:35:00,105\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:35:00,105\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:35:17,037\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.691]\n",
      "[\u001b[032m2022-02-18 12:35:50,848\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:35:50,851\u001b[0m INFO] trainer.training_epoch Training epoch 77, num_steps 3042, avg_loss: 0.6939, total_loss: 27.0615\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.16it/s]\n",
      "[\u001b[032m2022-02-18 12:35:53,057\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:35:53,058\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:36:09,958\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.694]\n",
      "[\u001b[032m2022-02-18 12:36:43,637\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:36:43,640\u001b[0m INFO] trainer.training_epoch Training epoch 78, num_steps 3081, avg_loss: 0.6948, total_loss: 27.0974\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 12:36:45,707\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:36:45,708\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:37:02,556\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.704]\n",
      "[\u001b[032m2022-02-18 12:37:36,388\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:37:36,391\u001b[0m INFO] trainer.training_epoch Training epoch 79, num_steps 3120, avg_loss: 0.6925, total_loss: 27.0089\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.45it/s]\n",
      "[\u001b[032m2022-02-18 12:37:38,459\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:37:38,460\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:37:55,224\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.17it/s, loss=0.698]\n",
      "[\u001b[032m2022-02-18 12:38:28,690\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:38:28,693\u001b[0m INFO] trainer.training_epoch Training epoch 80, num_steps 3159, avg_loss: 0.6948, total_loss: 27.0991\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 12:38:30,761\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5487364620938628), ('accuracy', 0.5487364620938628)])\n",
      "[\u001b[032m2022-02-18 12:38:30,761\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:38:47,920\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.7]  \n",
      "[\u001b[032m2022-02-18 12:39:21,577\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:39:21,580\u001b[0m INFO] trainer.training_epoch Training epoch 81, num_steps 3198, avg_loss: 0.6939, total_loss: 27.0636\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.45it/s]\n",
      "[\u001b[032m2022-02-18 12:39:23,648\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:39:23,649\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:39:41,354\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.701]\n",
      "[\u001b[032m2022-02-18 12:40:15,136\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:40:15,139\u001b[0m INFO] trainer.training_epoch Training epoch 82, num_steps 3237, avg_loss: 0.6963, total_loss: 27.1543\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.40it/s]\n",
      "[\u001b[032m2022-02-18 12:40:17,221\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:40:17,222\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:40:33,798\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.693]\n",
      "[\u001b[032m2022-02-18 12:41:07,611\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:41:07,614\u001b[0m INFO] trainer.training_epoch Training epoch 83, num_steps 3276, avg_loss: 0.6946, total_loss: 27.0879\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.39it/s]\n",
      "[\u001b[032m2022-02-18 12:41:09,707\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:41:09,708\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:41:26,017\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.697]\n",
      "[\u001b[032m2022-02-18 12:41:59,781\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:41:59,783\u001b[0m INFO] trainer.training_epoch Training epoch 84, num_steps 3315, avg_loss: 0.6938, total_loss: 27.0599\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.49it/s]\n",
      "[\u001b[032m2022-02-18 12:42:01,827\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:42:01,828\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:42:18,461\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.699]\n",
      "[\u001b[032m2022-02-18 12:42:52,126\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:42:52,129\u001b[0m INFO] trainer.training_epoch Training epoch 85, num_steps 3354, avg_loss: 0.6952, total_loss: 27.1130\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.16it/s]\n",
      "[\u001b[032m2022-02-18 12:42:54,335\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:42:54,336\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:43:11,250\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.682]\n",
      "[\u001b[032m2022-02-18 12:43:44,910\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:43:44,913\u001b[0m INFO] trainer.training_epoch Training epoch 86, num_steps 3393, avg_loss: 0.6939, total_loss: 27.0639\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.39it/s]\n",
      "[\u001b[032m2022-02-18 12:43:47,003\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:43:47,004\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:44:03,895\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.692]\n",
      "[\u001b[032m2022-02-18 12:44:37,884\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:44:37,886\u001b[0m INFO] trainer.training_epoch Training epoch 87, num_steps 3432, avg_loss: 0.6940, total_loss: 27.0672\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.10it/s]\n",
      "[\u001b[032m2022-02-18 12:44:40,143\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:44:40,143\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:44:56,729\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.69] \n",
      "[\u001b[032m2022-02-18 12:45:30,380\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:45:30,382\u001b[0m INFO] trainer.training_epoch Training epoch 88, num_steps 3471, avg_loss: 0.6953, total_loss: 27.1152\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 12:45:32,454\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:45:32,454\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:45:50,163\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.698]\n",
      "[\u001b[032m2022-02-18 12:46:24,009\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:46:24,012\u001b[0m INFO] trainer.training_epoch Training epoch 89, num_steps 3510, avg_loss: 0.6947, total_loss: 27.0938\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.41it/s]\n",
      "[\u001b[032m2022-02-18 12:46:26,092\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:46:26,093\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:46:43,104\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.686]\n",
      "[\u001b[032m2022-02-18 12:47:16,828\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:47:16,830\u001b[0m INFO] trainer.training_epoch Training epoch 90, num_steps 3549, avg_loss: 0.6942, total_loss: 27.0727\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.41it/s]\n",
      "[\u001b[032m2022-02-18 12:47:18,910\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:47:18,911\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:47:35,802\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.687]\n",
      "[\u001b[032m2022-02-18 12:48:09,566\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:48:09,569\u001b[0m INFO] trainer.training_epoch Training epoch 91, num_steps 3588, avg_loss: 0.6938, total_loss: 27.0599\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.45it/s]\n",
      "[\u001b[032m2022-02-18 12:48:11,633\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:48:11,634\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:48:28,263\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.699]\n",
      "[\u001b[032m2022-02-18 12:49:02,056\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:49:02,058\u001b[0m INFO] trainer.training_epoch Training epoch 92, num_steps 3627, avg_loss: 0.6939, total_loss: 27.0604\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.42it/s]\n",
      "[\u001b[032m2022-02-18 12:49:04,155\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:49:04,156\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:49:21,872\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.698]\n",
      "[\u001b[032m2022-02-18 12:49:55,730\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:49:55,732\u001b[0m INFO] trainer.training_epoch Training epoch 93, num_steps 3666, avg_loss: 0.6955, total_loss: 27.1234\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.35it/s]\n",
      "[\u001b[032m2022-02-18 12:49:57,848\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:49:57,848\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:50:14,476\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.695]\n",
      "[\u001b[032m2022-02-18 12:50:48,262\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:50:48,263\u001b[0m INFO] trainer.training_epoch Training epoch 94, num_steps 3705, avg_loss: 0.6952, total_loss: 27.1145\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.43it/s]\n",
      "[\u001b[032m2022-02-18 12:50:50,333\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:50:50,334\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:51:06,932\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.16it/s, loss=0.692]\n",
      "[\u001b[032m2022-02-18 12:51:40,624\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:51:40,627\u001b[0m INFO] trainer.training_epoch Training epoch 95, num_steps 3744, avg_loss: 0.6959, total_loss: 27.1397\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.19it/s]\n",
      "[\u001b[032m2022-02-18 12:51:42,822\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:51:42,822\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:51:59,411\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:33<00:00,  1.15it/s, loss=0.694]\n",
      "[\u001b[032m2022-02-18 12:52:33,233\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:52:33,236\u001b[0m INFO] trainer.training_epoch Training epoch 96, num_steps 3783, avg_loss: 0.6934, total_loss: 27.0436\n",
      "validation: 100%|██████████| 9/9 [00:02<00:00,  4.39it/s]\n",
      "[\u001b[032m2022-02-18 12:52:35,325\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5126353790613718), ('accuracy', 0.5126353790613718)])\n",
      "[\u001b[032m2022-02-18 12:52:35,325\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:52:51,958\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.22it/s, loss=0.689]\n",
      "[\u001b[032m2022-02-18 12:53:23,926\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:53:23,928\u001b[0m INFO] trainer.training_epoch Training epoch 97, num_steps 3822, avg_loss: 0.6946, total_loss: 27.0883\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.53it/s]\n",
      "[\u001b[032m2022-02-18 12:53:25,957\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:53:25,958\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:53:42,530\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:31<00:00,  1.23it/s, loss=0.692]\n",
      "[\u001b[032m2022-02-18 12:54:14,329\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:54:14,330\u001b[0m INFO] trainer.training_epoch Training epoch 98, num_steps 3861, avg_loss: 0.6942, total_loss: 27.0740\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.90it/s]\n",
      "[\u001b[032m2022-02-18 12:54:16,208\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:54:16,211\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:54:32,800\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "100%|██████████| 39/39 [00:32<00:00,  1.22it/s, loss=0.7]  \n",
      "[\u001b[032m2022-02-18 12:55:04,812\u001b[0m INFO] trainer.training_epoch Global step 39\n",
      "[\u001b[032m2022-02-18 12:55:04,815\u001b[0m INFO] trainer.training_epoch Training epoch 99, num_steps 3900, avg_loss: 0.6941, total_loss: 27.0687\n",
      "validation: 100%|██████████| 9/9 [00:01<00:00,  4.86it/s]\n",
      "[\u001b[032m2022-02-18 12:55:06,707\u001b[0m INFO] trainer.inference_epoch validation Performance: OrderedDict([('micro-f1', 0.5270758122743683), ('accuracy', 0.5270758122743683)])\n",
      "[\u001b[032m2022-02-18 12:55:06,708\u001b[0m INFO] trainer.save_checkpoint Saving checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/last.ckpt...\n",
      "[\u001b[032m2022-02-18 12:55:23,409\u001b[0m INFO] trainer.save_checkpoint Save Checkpoint finished\n",
      "[\u001b[032m2022-02-18 12:55:23,413\u001b[0m INFO] trainer.load_checkpoint Loading Checkpoint logs/rte_roberta-large_soft_template_manual_verbalizer_0218111901805843/checkpoints/best.ckpt...\n",
      "[\u001b[032m2022-02-18 12:55:24,925\u001b[0m INFO] trainer.load_checkpoint Load Checkpoint finished, the current validation metric: 0.8844765342960289\n",
      "test: 100%|██████████| 9/9 [00:06<00:00,  1.41it/s]\n",
      "[\u001b[032m2022-02-18 12:55:31,350\u001b[0m INFO] trainer.inference_epoch test Performance: OrderedDict([('micro-f1', 0.8844765342960289), ('accuracy', 0.8844765342960289)])\n",
      "[\u001b[032m2022-02-18 12:55:31,381\u001b[0m INFO] 878160906.main the best result of test Performance:( micro-f1: 0.8844765342960289, accuracy: 0.8844765342960289)\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('logs', exist_ok=True)\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0f638c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
